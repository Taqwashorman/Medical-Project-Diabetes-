# -*- coding: utf-8 -*-
"""assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u9N_trD9SkZ9rvpICyKn9AShz1Hi9HiO
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

df = pd.read_csv('/content/diabetes.csv')
df

"""#EDA"""

df.shape

df.dtypes

df.isnull().sum()

df.duplicated().sum()

df.describe()

correlation = df.corr()['Outcome'].abs()
correlation

#The code checks the frequency and unique values of each column in the dataset `df`, helping to identify data distribution and issues.
for col in df.columns:
    print(f"Column: {col}")
    print("Unique values:", df[col].nunique())
    print(df[col].value_counts())
    print("------------")

#boxplot of each column before Data Preprocessing
for column in df.columns:
    plt.figure(figsize=(6, 4))
    df.boxplot(column=column)
    plt.title(f"Boxplot of {column}")
    plt.show()

"""#Data Cleaning & Preprocessing"""

#The code replaces rows with `Glucose` values of 0 with the median `Glucose` value, addressing potential data issues.
print(df["Glucose"].value_counts()[0])
zero_glucose_rows = df[df["Glucose"] == 0]
print(zero_glucose_rows)
glucose_median = df["Glucose"].median()
df["Glucose"] = df["Glucose"].replace(0, glucose_median)

#The code replaces rows with `BloodPressure` values of 0 with the median `BloodPressure` value, addressing potential data issues.
print(df["BloodPressure"].value_counts()[0])
zero_BloodPressure_rows = df[df["BloodPressure"] == 0]
print(zero_BloodPressure_rows)
BloodPressure_median = df["BloodPressure"].median()
df["BloodPressure"] = df["BloodPressure"].replace(0, BloodPressure_median)

#The column `SkinThickness` is dropped due to its weak correlation with the target variable or other features, which means it doesn't contribute much to the model's performance.
df_cleaned = df.drop(columns='SkinThickness')

#The code replaces rows with `Insulin` values of 0 with the median `Insulin` value, addressing potential data issues.
zero_Insulin_rows = df[df["Insulin"] == 0]
Insulin_median = df["Insulin"].median()
df["Insulin"] = df["Insulin"].replace(0, Insulin_median)

# The code replaces `Insulin` values above the upper bound (1.5 * IQR) with the upper bound to handle outliers.
Q1 = df['Insulin'].quantile(0.25)
Q3 = df['Insulin'].quantile(0.75)
IQR = Q3 - Q1
upper_bound = Q3 + 1.5 * IQR
print(f"upper_bound: {upper_bound}")
df['Insulin'] = np.where(df['Insulin'] > upper_bound, upper_bound, df['Insulin'])

#The code replaces rows with `BMI` values of 0 with the median `BMI` value, addressing potential data issues.
zero_BMI_rows = df[df["BMI"] == 0]
BMI_median = df["BMI"].median()
df["BMI"] = df["BMI"].replace(0, BMI_median)

#Boxplot after Data Preprocessing
for column in df.columns:
    plt.figure(figsize=(6, 4))
    df.boxplot(column=column)
    plt.title(f"Boxplot of {column}")
    plt.show()

"""#Model Building"""

X=df.drop('Outcome',axis=1)
y=df['Outcome']

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Feature scaling with MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

# Handle class imbalance with SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)

# Define an expanded hyperparameter grid
param_grid = {
    'C': [0.1, 1, 10,],
    'gamma': ['scale', 0.1, 1],
    'kernel': ['rbf', 'poly', 'linear'],
    'degree': [2, 3],
    'coef0': [0, 1, 10]
}

# Train SVM with GridSearchCV and higher CV
svm_model = SVC()
grid = GridSearchCV(svm_model, param_grid, cv=5, scoring='accuracy', verbose=1)
grid.fit(X_train, y_train)

# Best model
best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)

# Predict and evaluate
y_pred = best_model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

